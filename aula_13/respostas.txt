Schedulers

default: time: 0.000201159, time: 0.000228217, time: 0.000213511
schedule(static): time: 2.82815e-05, time: 3.27304e-05, time: 2.78624e-05
schedule(static, 4): time: 2.7434e-05, time: 3.16883e-05, time: 2.79173e-05
schedule(static, 8): time: 2.70838e-05, time: 3.60059e-05, time: 2.7366e-05
schedule(dynamic): time: 3.0973e-05, time: 3.50866e-05, time: 3.16333e-05
schedule(dynamic, 1): time: 2.95518e-05, time: 3.71123e-05, time: 3.66932e-05
schedule(dynamic, 4): time: 2.85264e-05, time: 3.26606e-05, time: 2.93115e-05
schedule(dynamic, 8):  time: 2.8084e-05, time: 3.26093e-05, time: 2.89762e-05
schedule(guided): time: 2.99355e-05, time: 3.38256e-05, time: 2.99839e-05
schedule(guided, 2): time: 3.66522e-05, time: 3.2953e-05, time: 2.90088e-05
schedule(guided, 4): time: 2.76798e-05, time: 3.25078e-05, time: 2.83467e-05
schedule(guided, 8): time: 3.33879e-05, time: 3.25441e-05, time: 2.84556e-05
schedule(auto): time: 3.15579e-05, time: 3.08026e-05, time: 2.84594e-05
schedule(runtime): time: 3.38117e-05, time: 3.38862e-05, time: 3.13288e-05



PI

cálculo do pi paralelo
for 2147483648 steps pi = 3.14159265358979 in 1.07949697133154 secs (MIN_BLK  1024*1024*256)
for 2147483648 steps pi = 3.14159265358978 in 1.08087346423417 secs (MIN_BLK  1024*1024)
for 2147483648 steps pi = 3.14159265358996 in 4.51369129959494 secs (MIN_BLK  1024)

cálculo do pi em tasks 
for 2147483648 steps pi = 3.14159265358983 in 1.34980638604611 secs (num_steps = 1024l*1024*1024*2)   
for 1048576 steps pi = 3.14159265358979 in 0.00188430026173592 secs (num_steps = 1024l*1024)
for 1024 steps pi = 3.14159273306265 in 0.000144052319228649 secs (num_steps = 1024l)

cálculo do pi com task e parallel
for 2147483648 steps pi = 3.14159265358983 in 1.34738512989134 secs (num_steps = 1024l*1024*1024*2)
for 1048576 steps pi = 3.14159265358979 in 0.0022222176194191 secs (num_steps = 1024l*1024)
for 1024 steps pi = 3.14159273306265 in 0.000123710371553898 secs (num_steps = 1024l)

O uso da diretiva task resultou em códigos mais rápidos, porém com valores mais distântes de pi a medida que o
número de passos do cálculo "integral" diminui. Seria recomendado para tarefar cujo valor aproximado de pi, com 
pucas casas decimais, já é sufuciente.
Já a diretiva parallel sozinha se mostrou eficiente e contante para o cálculo de pi (não mudei o valor dos passos)
mas a alteração do MIN_BLK teve impacto significativo no tempo de processamento, o que já era esperado, pois um MIN_BLK
pequeno pode resultar em uma quantidade excessiva de threads, resultando em overhead.
Claramente o uso de tasks com diferentes passos resultou em uma diferença mais significante, já que esse número está
diretamente ligado ao cálculo do pi.



MANIPULAÇÃO DE EFEITOS COLATERAIS

com a diretriz critical
time: 0.00118872
time: 0.00132442
time: 0.00173004

com alocação de memória
time: 0.00047549
time: 0.000577642
time: 0.000505978


Fica claro aqui que a pré-alocação de memória se mostrou mais efetiva, pelo menos para pequenos valores, sendo
cercad e 10 vezes mais rápida que o uso de região crítica.
Acredito que o uso de uma região critica, fazendo cada thread rodar uma de cada vez, reduzindo o paralelismo, e 
para um cálculo tão simples, quanto multiplicar um valor por 2, culmina em uma alocação indevida de recursos de processamento, ou seja, em
um overhead considerável.
Em ambas as abordagens o vetor é mantido na ordem, na com região critica porque as threads respeitam o tempo de cada
uma terminar para outra começar (sequencial) e na prealocação, mesmo que o vetor seja preenchido de maneira 
aleatória, a ordem dos indices se mantém.

